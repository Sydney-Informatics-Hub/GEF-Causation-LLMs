{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c51d80-ed1e-4757-ab30-969fa639a41d",
   "metadata": {},
   "source": [
    "# GEF: GPT-turbo-3.5 zero-shot classification\n",
    "\n",
    "This notebook uses zero-shot classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379e5dd-1cc6-4c31-9dae-0c868aee93d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OpenAI Privacy Policy\n",
    "This notebook uses OpenAI's API, meaning that your data will be sent to the OpenAI servers.\n",
    "\n",
    "For concerns about how your data will be handled, please read through the Privacy Policy [here](https://openai.com/policies/api-data-usage-policies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a039fc3-f805-4823-9b9a-be7a092a4da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from causation.utils import openai_apikey_input\n",
    "\n",
    "openai_apikey_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068765ca-d45c-4100-aa17-749a75939b46",
   "metadata": {},
   "source": [
    "# Upload Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c498a8-70be-4ece-bcd4-538d3a3f760b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from causation.utils import fileuploader \n",
    "\n",
    "finput, uploaded = fileuploader('.toml')\n",
    "finput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e78fd7-98f5-4cae-8836-9d854026b743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "import toml\n",
    "from pathlib import Path\n",
    "\n",
    "path = uploaded.get('data', None)\n",
    "assert path is not None, \"Did you upload your .toml file?\"\n",
    "\n",
    "path = Path(path)\n",
    "if not path.suffix == '.toml': raise ValueError(\"path is not a toml file.\")\n",
    "import toml\n",
    "data = toml.load(path)\n",
    "\n",
    "if \"PREFIX\" in data.keys():\n",
    "    prefix = data.pop(\"PREFIX\")\n",
    "    prefix_instructions = prefix.get('instruction', '')\n",
    "else:\n",
    "    prefix_instructions = ''\n",
    "\n",
    "classes = list(data.keys())\n",
    "instructions = []\n",
    "for clz in classes:\n",
    "    instruction = data.get(clz).get('instruction')\n",
    "    instruction = f\"<class>\\n{clz}: {instruction}</class>\"\n",
    "    instructions.append(instruction)\n",
    "\n",
    "instruction = prefix_instructions + \"\\n\\n\" + f\"\"\"\n",
    "The following are {len(classes)} classes with a description of each. \n",
    "These are XML delimited with <class> tags in the format: <class> Class: Description </class>.\n",
    "Please classify each 'query' as one of the {len(classes)} classes.\\n\\n\"\"\" + '\\n'.join(instructions) + \"\\n\\n\"\n",
    "\n",
    "instruction += \"\\n\\n{format_instructions}\\nQuery: {query}\"\n",
    "template = PromptTemplate.from_template(instruction)\n",
    "template\n",
    "\n",
    "class ClassificationOutput(BaseModel):\n",
    "    answer: str = Field(description=\"the classification\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=ClassificationOutput)\n",
    "\n",
    "# template = load_prompt(exemplars.get('data'))\n",
    "template = template.partial(format_instructions=parser.get_format_instructions())\n",
    "human = HumanMessagePromptTemplate(prompt=template)\n",
    "chat = ChatPromptTemplate.from_messages(messages=[human])\n",
    "\n",
    "print(f\"Classes found: {', '.join(classes)}\")\n",
    "\"Prompt set up complete. Please continue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e5699-d7da-4452-bea7-0ac1d3d6e6ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo', \n",
    "    n=1,\n",
    "    temperature=0.0,\n",
    "    model_kwargs={'top_p': 0.8},\n",
    ")\n",
    "\n",
    "\"LLM set up complete. Please continue.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1d71bc-aa95-417e-8891-2b56594b7d8f",
   "metadata": {},
   "source": [
    "# Upload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e89bb-9fbb-4e47-b19d-30ee48daad60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from causation.utils import fileuploader\n",
    "\n",
    "finput, dataset = fileuploader('.xlsx')\n",
    "finput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c23456-43ca-4f69-be8e-629517fe6451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "assert dataset.get('data'), \"Did you upload your dataset?\"\n",
    "df = pd.read_excel(dataset.get('data'))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244841fb-68af-448c-bb50-fed55673b0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "checkpointing = 200\n",
    "corrupted = []\n",
    "classifications = []\n",
    "for i, sent in tqdm(enumerate(df.sentence), total=len(df)):\n",
    "    messages = chat.format_prompt(query=sent).to_messages()\n",
    "    results = llm.generate([messages])\n",
    "    try:\n",
    "        answer = parser.parse(results.generations[0][0].text).answer\n",
    "        classifications.append((sent, answer))\n",
    "    except:\n",
    "        print(\"Got corrupted llm output. These are added to an excel sheet so you can rerun these later.\")\n",
    "        corrupted.append(sent)\n",
    "        \n",
    "    if checkpointing and (i + 1) % checkpointing == 0:\n",
    "        path = f'./cotsc-outputs-checkpoint-{i + 1}.xlsx'\n",
    "        pd.DataFrame(classifications, columns=['sentence', 'classification']).to_excel(path)\n",
    "        print(f\"Checkpointed at {i + 1} queries processed. Checkpoint file: {path}.\")\n",
    "        \n",
    "f\"Passed {len(classifications)}/{len(df)}. Please continue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c3fe7e-41aa-4bf3-8182-75f636d88809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrupted_df = pd.DataFrame(corrupted, columns=['sentence'])\n",
    "results_df = pd.DataFrame(classifications, columns=['sentence', 'classification'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e601852a-5bb6-4dc0-a7d9-a734fb271fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import srsly\n",
    "\n",
    "now = datetime.now().strftime(format=\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_dir = Path(f\"./.zeroshot-corpus-output-{now}\")\n",
    "output_dir.mkdir(exist_ok=False)\n",
    "\n",
    "results_df.to_excel(output_dir.joinpath('zeroshot-output.xlsx'))\n",
    "path = output_dir.joinpath('zeroshot-config.json')\n",
    "config = {\n",
    "    'model': llm.model_name,\n",
    "    'temperature': llm.temperature,\n",
    "    'top_p': llm.model_kwargs.get('top_p', 'N/A'),\n",
    "    'n_completions': llm.n,\n",
    "}\n",
    "srsly.write_json(path, config)\n",
    "\n",
    "if len(corrupted_df) > 0:\n",
    "    corrupted_df.to_excel(output_dir.joinpath('zeroshot-corrupted.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a191a4-b147-449d-ba57-569e85ad08ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_names = [uploaded['data'], dataset['data']]\n",
    "file_names += list(output_dir.glob(\"*\"))\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab718843-9d18-4e9d-8d12-c34bbae950aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import panel as pn\n",
    "\n",
    "zfname = Path(f'{now}-zeroshot-corpus.zip')\n",
    "with zipfile.ZipFile(zfname, 'w') as zipf:\n",
    "    for file_name in file_names:\n",
    "        zipf.write(file_name, arcname=os.path.basename(file_name))\n",
    "print(f\"Saved as {zfname}.\\nClick below to download.\")\n",
    "\n",
    "# download link for the zip.\n",
    "pn.widgets.FileDownload(file=str(zfname), filename=zfname.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

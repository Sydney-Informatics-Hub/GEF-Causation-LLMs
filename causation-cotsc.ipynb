{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06444d10-e246-4eb6-b2fc-ba294bd6748f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Causation (Chain of Thoughts Self Consistency)\n",
    "\n",
    "Chain of Thoughts is a prompting technique that use examples containing steps to assist with the reasoning ability of a large language model (llm).\n",
    "\n",
    "Self consistency is a layer that adds on top that leverage the probabilistic outputs from LLMs and take the majority vote as the final answer.\n",
    "\n",
    "This notebook is an implementation of this for the GEF causation project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42c79c-3385-46bc-9d5f-c966da6c7305",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Paper: Chain of Thoughts (expand to read)\n",
    "Chain of Thought (CoT): https://arxiv.org/abs/2201.11903<br>\n",
    "CoT Self Consistency (CoT-SC): https://arxiv.org/abs/2203.11171"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc72e9f-ac88-4d34-9929-6bd73cdbcb3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "display(IFrame(src='https://arxiv.org/pdf/2201.11903.pdf', width=1600, height=700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9842cdf-2083-4177-8708-1c5cb4430787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "# CoT - Self Consistency\n",
    "display(IFrame(src='https://arxiv.org/pdf/2203.11171.pdf', width=1600, height=700))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a675e4-bda1-48ee-9c60-575f51eab885",
   "metadata": {},
   "source": [
    "## Overview\n",
    "1. Supply your OpenAI API Key\n",
    "2. Choose a sampling scheme and number of completions\n",
    "3. Upload your file with the Chain of Thoughts examples.\n",
    "4. Enter a query (i.e. the sentence) and run it for a classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed9d9e9-3880-4194-8c48-0d14a3f8fb1e",
   "metadata": {},
   "source": [
    "## 1. Enter your Open API Key.\n",
    "Replace `<OPENAI_API_KEY>` below and retain the double quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebcd210-632b-4b3b-b9d1-726ee80a6a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"<OPENAI_API_KEY>\"\n",
    "\n",
    "# ignore this: it just sets your key onto the environment variable (disappears after you close it or restart the notebook)\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d4eda-3939-4f8b-869c-065383e2b2e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Choose a sampling scheme and the number of completions.\n",
    "\n",
    "__Sampling Scheme__:\n",
    "+ `top_p` - **the range of output words within a probability threshold.** (0 < top_p <= 1.0)<br>\n",
    "e.g. <br>\n",
    "= 0.3 means sample only from outputs that make up the top 30% of the probabilities.\n",
    "\n",
    "+ `temperature` - **the higher the temperature the more spread out the probabilities are across the output words.** (0 <= temperature <= 2) <br>\n",
    "e.g. <br>\n",
    "= 0 means output probabilities are *not* spread out i.e. Only 1 output token have a probability of 1.0 when the LLM is generating the output. This means it will always choose the same output during the decoding ending up with the same completion.<br>\n",
    "\\> 0, ~0 means output probabilities are a little spread out i.e. > 1 output tokens will have a probability of closer to 1.0, combining to 1.0. This means that the LLM will have a chance of sampling different output tokens.<br>\n",
    "= 2 means output probabilities are *most* spread out i.e. output tokens will have similar probabilities and the LLM will have similar chance of sampling from each token.\n",
    "\n",
    "`top_p` and `temperature` goes hand in hand. Having a really low temperature means there are less output tokens within the probability threshold.\n",
    "\n",
    "__Number of completions__ (relates to: self-consistency):\n",
    "+ `n_completions` - **this is the number of responses you're asking the LLM to generate.**<br>(increases cost but directly relates to the number of votes used by Self Consistency)\n",
    "\n",
    "<br>\n",
    "ðŸ‘¼ Experiment with different sampling schemes and increase number of completions for more confidence in your votes (beware of costs!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8006703e-cb0f-45a5-a288-4eebe0b6de22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llm_experiments import SamplingScheme\n",
    "\n",
    "sampling_scheme = SamplingScheme(top_k=None, top_p=0.2, temperature=1)    # top_k is None because OpenAI does not support it.\n",
    "n_completions = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ad013-f7f0-42c2-8f8c-f03f3d30a019",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Upload your Chain of Thoughts examples\n",
    "\n",
    "Chain of thoughts uses examples to help the LLM reason about your queries better by breaking it down into steps.\n",
    "\n",
    "The idea is to give it more context and prompt it to break down its reasoning process into steps.\n",
    "\n",
    "ðŸ‘¼ Experiment with different 'steps' for each example, I would recommend having a diverse set of reasons and be specific. <br> \n",
    "ðŸ‘¼ It's usually better with more examples but this also means it'll increase your API cost (see TikDollar later). Starting out with 3-5 examples per class should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9610eb5b-42a2-4f43-958b-7ea7ef0b2c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tempfile import mkdtemp\n",
    "from pathlib import Path\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "MARKDOWN = str\n",
    "\n",
    "uploader_data = dict(saved_path=None)\n",
    "\n",
    "def cb_save_to_file(fbytes: bytes, fname: str) -> MARKDOWN:\n",
    "    if fbytes is None or len(fbytes) <= 0 or fname is None: return \"\"\n",
    "    dir_ = Path(mkdtemp())\n",
    "    path = dir_.joinpath(fname)\n",
    "    with open(path, 'wb') as h:\n",
    "        h.write(finput.value)\n",
    "    uploader_data['saved_path'] = path\n",
    "    return f\"Received: **{fname}**\\t\\tSaved temporarily to **{uploader_data.get('saved_path')}**\"\n",
    "\n",
    "finput = pn.widgets.FileInput(accept='.toml')\n",
    "iobject = pn.bind(cb_save_to_file, finput, finput.param.filename)\n",
    "pn.Row(finput, pn.pane.Markdown(iobject))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bac09-53bf-4cdd-804a-f74f11b627a9",
   "metadata": {},
   "source": [
    "ðŸ‘¼ Currently supported models: text-davinci-003, text-davinci-002, text-curie-001, text-babbage-001, text-ada-001\n",
    "(In decreasing performance and cost.)\n",
    "\n",
    "Note: If you enter the wrong model name, it'll tell you what's available in the error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab8662d-cac0-4ee9-8924-8b9161ed12cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llm_experiments import CoTSC\n",
    "\n",
    "assert uploader_data.get('saved_path'), \"Have you uploaded your Chain of Thoughts examples .toml configuration file?\"\n",
    "\n",
    "cotsc = CoTSC.from_toml(model='text-davinci-002',\n",
    "                        prompt_toml=uploader_data.get('saved_path'),\n",
    "                        sampling_scheme=sampling_scheme, \n",
    "                        n_completions=n_completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02bf731-1816-436c-99a6-83ea5005d84d",
   "metadata": {},
   "source": [
    "ðŸ§‘â€ðŸ’» Side Note: TOML is a modern standard used often to define configurations in a file for programs. It works well with python and is generally considered to supercede the YAML standard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67bd4f-f4a1-4ab6-9800-f8c0d93741b5",
   "metadata": {},
   "source": [
    "### Here's an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6454085e-20d4-4052-b22d-41eec510f618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = cotsc.run(query=\"Canberra immunologist Carola Vinuesa who discovered a gene responsible for the autoimmune diseases lupus and diabetes.\")\n",
    "results  # raw output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08352823-a828-4900-8a04-1a7e1f39673b",
   "metadata": {},
   "source": [
    "We'll format the raw output a little bit by putting it in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b9777-d3bd-4c45-895e-9c9b6a2b701e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "\n",
    "pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66361306-65f2-4529-8b19-7b88fadefd52",
   "metadata": {},
   "source": [
    "### TikDollar\n",
    "**Tikdollar is created by SIH to track your OpenAI expense and cut if off at a specified threshold.<br>\n",
    "The code will run your calls right up until when your next call will exceed this threshold.**\n",
    "\n",
    "> OpenAI charges for both your input and output tokens. Each token can be thought of as a word in the normal sense but tokens for LLMs are actually *subwords*.\n",
    "\n",
    "For more information on subwords here's a cool video: https://www.youtube.com/watch?v=zHvTiHr506c\n",
    "\n",
    "**Now, we're going to bind TikDollar with our CoTSC calling function.**\n",
    "\n",
    "The parameters you need to care about:\n",
    "+ `cost_threshold`  - this is the cut-off USD. You'll need to define this. (e.g. 0.1, 1.0, 20)\n",
    "+ `raise_err` - when the cut-off will be exceeded in the next call, stop. (or print a message then continue if it's False)\n",
    "+ `verbose` - whether to print messages per call in terms of your spending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06feb66e-4b80-4452-95a2-41acabd9e790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llm_experiments.utils import TikDollar as td\n",
    "\n",
    "# âš ï¸ Caveat: When you rerun this cell, tikdollar is reset to 0!\n",
    "tikdollar = td.track(cotsc, cotsc._tikdollar_run, cost_threshold=0.1, raise_err=True, verbose=True)\n",
    "tikdollar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca295c7-3beb-4336-8f99-f4a540b9fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cotsc.run(query=\"Professor Lawford says the institute, based at the Greenslopes Private Hospital, in Brisbane's south, in conjunction with overseas collaborators, is analysing the DNA of Australian Vietnam veterans in a bid to better understand the causes of PTSD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277a86e-7d92-494d-9772-e694fe7eaf47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tikdollar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5bac93-8caa-446c-83fa-58ab24cfbaad",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Now that you have **CoTSC** and **TikDollar** you're equipped to run a classification task on your list of sentences!\n",
    "\n",
    "At the end of this there'll be a link for you to click on to download all the generated results into an excel sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa1a8f-9fb3-4f7b-ba74-5f04da458f55",
   "metadata": {},
   "source": [
    "#### 1. Upload your dataset (requires: 'sentence' column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3edb2-5eb9-4225-8593-9da2f92aa320",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploader_data.clear()\n",
    "finput = pn.widgets.FileInput(accept='.xlsx')\n",
    "iobject = pn.bind(cb_save_to_file, finput, finput.param.filename)\n",
    "pn.Row(finput, pn.pane.Markdown(iobject))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f59ab-b379-4139-9fb7-74c65165e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "assert uploader_data.get('saved_path'), \"Did you upload your dataset?\"\n",
    "df = pd.read_excel(uploader_data.get('saved_path'))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747414c4-e282-48e0-bb02-e760efc0849c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['det', 'se', 'nat', 'hom']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e524234f-ddfb-4648-8526-a1ee99418012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703cb415-29d4-4bb6-88b2-936660ce3131",
   "metadata": {},
   "source": [
    "#### 2. Set up TikDollar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e435f1a0-643d-4220-887d-9282f65f86ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup TikDollar\n",
    "# copy of prior cell for task separation and easy access.\n",
    "from llm_experiments.utils import TikDollar as td\n",
    "\n",
    "# âš ï¸ Caveat: When you rerun this cell, tikdollar is reset to 0!\n",
    "tikdollar = td.track(cotsc, cotsc._tikdollar_run, cost_threshold=5, raise_err=True, verbose=True)\n",
    "tikdollar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6577da-b604-4747-8dbf-32f00ec529a6",
   "metadata": {},
   "source": [
    "#### 3. Run classification on uploaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd145a82-d21e-4c6e-9de4-51e60be85462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_experiments.utils.tikdollar import CostThresholdReachedException\n",
    "from collections import namedtuple\n",
    "ROW = namedtuple('ROW', ['query', 'clazz', 'votes', 'steps', 'determinism', 'specific_aetiology', 'naturalness', 'homogeneity', 'is_biased', 'completions'])\n",
    "\n",
    "Rows = list()\n",
    "for i, row in enumerate(df.sample(10).itertuples()):\n",
    "    query = row.sentence\n",
    "    det, se, nat, hom, pos = row.det, row.se, row.nat, row.hom, row.pos\n",
    "    try:\n",
    "        results = cotsc.run(query=query)\n",
    "        for clazz, clz_res in results.items():\n",
    "            Row = ROW(query=query, clazz=clazz, votes=clz_res.get('votes'), steps=clz_res.get('steps'), \n",
    "                      determinism=det, specific_aetiology=se, naturalness=nat, homogeneity=hom, is_biased=pos, completions=clz_res.get('completions'))\n",
    "            Rows.append(Row)\n",
    "    except CostThresholdReachedException as ctre:\n",
    "        print(ctre)\n",
    "        print(f\"Number of queries sent: {i}.\")\n",
    "        break\n",
    "              \n",
    "results_df = pd.DataFrame(Rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05595b-d4c8-4824-a924-7e8a711820d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(Rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd9507-8edc-4f76-be52-f8600adefa93",
   "metadata": {},
   "source": [
    "#### 4. Analyse classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2235d923-d630-4f48-84ef-3fe1d8ab5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d13ad-da82-4f67-8f73-da87bfd58598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "clazz_to_idx = {\n",
    "    \"determinism\": 0,\n",
    "    \"specific_aetiology\": 1,\n",
    "    \"naturalness\": 2,\n",
    "    \"homogeneity\": 3,\n",
    "}\n",
    "\n",
    "def clazz_to_row(clazz: str) -> list[int]:\n",
    "    # clean up llm output\n",
    "    try:\n",
    "        comma_idx = clazz.index(',')\n",
    "        clazz = clazz[:comma_idx]\n",
    "    except:\n",
    "        pass\n",
    "    clazz = clazz.strip()\n",
    "    row = [0, 0, 0, 0]\n",
    "    if clazz == 'not_biased': \n",
    "        return row\n",
    "    elif clazz_to_idx.get(clazz, None) is None:\n",
    "        print(f\"{clazz} is not one of {', '.join(clazz_to_idx.keys())}. Continued as 'not_biased'\", file=sys.stderr)\n",
    "        return row\n",
    "    else:\n",
    "        row[clazz_to_idx.get(clazz)] = 1\n",
    "        return row\n",
    "\n",
    "preds, targets = list(), list()\n",
    "for query, group in results_df.groupby(by='query'):\n",
    "    best_idx = group.loc[:, 'votes'].idxmax()\n",
    "    best = group.loc[best_idx]\n",
    "    prediction = clazz_to_row(best.clazz)\n",
    "    target = group[['determinism', 'specific_aetiology', 'naturalness', 'homogeneity']].values[0]\n",
    "    preds.append(prediction)\n",
    "    targets.append(target)\n",
    "\n",
    "preds, targets = np.array(preds), np.array(targets)\n",
    "assert preds.shape == targets.shape, \"Mismatched shape between prediction and targets. This should not happen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8127352-540e-449a-a701-9f0b207dfc83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clazzes = list(clazz_to_idx.keys())\n",
    "num_clazzes = preds.shape[1]\n",
    "for i in range(num_clazzes):\n",
    "    print(f\"=== {clazzes[i]} ===\")\n",
    "    print(classification_report(y_true=targets[:, i], y_pred=preds[:, i]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662fdda-fa0e-4422-bee8-04122ae2bd0c",
   "metadata": {},
   "source": [
    "#### 5. Download results as excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118629dd-565c-457b-85fc-01b183486838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reformat df for download\n",
    "formatted_results_df = results_df.copy()\n",
    "formatted_results_df['steps'] = formatted_results_df['steps'].apply(lambda steps: \"\\n+++++++\\n\".join((s for s in steps)))\n",
    "formatted_results_df['completions'] = formatted_results_df['completions'].apply(lambda completions: \"\\n+++++\\n\".join((c for c in completions)))\n",
    "formatted_results_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8914f-37f1-44a3-967d-98569fd698fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "\n",
    "path = 'cotsc-results.xlsx'\n",
    "formatted_results_df.to_excel(path, index=False)\n",
    "pn.widgets.FileDownload(file=path, filename=path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06444d10-e246-4eb6-b2fc-ba294bd6748f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Causation (Chain of Thoughts Self Consistency)\n",
    "\n",
    "Chain of Thoughts is a prompting technique that use examples containing steps to assist with the reasoning ability of a large language model (llm).\n",
    "\n",
    "Self consistency is a layer that adds on top that leverage the probabilistic outputs from LLMs and take the majority vote as the final answer.\n",
    "\n",
    "This notebook is an implementation of this for the GEF causation project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42c79c-3385-46bc-9d5f-c966da6c7305",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Paper: Chain of Thoughts (expand to read)\n",
    "Chain of Thought (CoT): https://arxiv.org/abs/2201.11903<br>\n",
    "CoT Self Consistency (CoT-SC): https://arxiv.org/abs/2203.11171"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc72e9f-ac88-4d34-9929-6bd73cdbcb3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "display(IFrame(src='https://arxiv.org/pdf/2201.11903.pdf', width=1600, height=700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9842cdf-2083-4177-8708-1c5cb4430787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "# CoT - Self Consistency\n",
    "display(IFrame(src='https://arxiv.org/pdf/2203.11171.pdf', width=1600, height=700))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a675e4-bda1-48ee-9c60-575f51eab885",
   "metadata": {},
   "source": [
    "## Overview\n",
    "1. Supply your OpenAI API Key\n",
    "2. Choose a sampling scheme and number of completions\n",
    "3. Upload your file with the Chain of Thoughts examples.\n",
    "4. Enter a query (i.e. the sentence) and run it for a classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e8db91-60c6-4119-b94a-805ccda02216",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OpenAI Privacy Policy\n",
    "This notebook uses OpenAI's API.\n",
    "\n",
    "For concerns about how your data will be handled, please read through the Privacy Policy here: https://openai.com/policies/api-data-usage-policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed9d9e9-3880-4194-8c48-0d14a3f8fb1e",
   "metadata": {},
   "source": [
    "## 1. Enter your OpenAI API Key.\n",
    "\n",
    "To create an API Key, please go to: https://platform.openai.com/account/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9067d-78a9-45bd-9179-2ec8a1f098bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enter your API Key via a redacted input box.\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "password_input = pn.widgets.PasswordInput(name='Enter your OpenAI API key then run the next cell:', placeholder='<OpenAI API Key>')\n",
    "password_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af084f-a51f-4c49-acff-e6643f910f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell once you've entered your API key.\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = password_input.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd051f-e63a-415d-8a18-e233050a8767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validate your API key.\n",
    "import re, os\n",
    "assert len(os.environ['OPENAI_API_KEY']) == 51, \"OpenAI's API Key are 51 characters.\"\n",
    "os.environ['OPENAI_API_KEY'][:3] + re.sub('.', '*', os.environ['OPENAI_API_KEY'][3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d4eda-3939-4f8b-869c-065383e2b2e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Choose a sampling scheme and the number of completions.\n",
    "\n",
    "__Sampling Scheme__:\n",
    "+ `top_p` - **the range of output words within a probability threshold.** (0 < top_p <= 1.0)<br>\n",
    "e.g. <br>\n",
    "= 0.3 means sample only from outputs that make up the top 30% of the probabilities.\n",
    "\n",
    "+ `temperature` - **the higher the temperature the more spread out the probabilities are across the output words.** (0 <= temperature <= 2) <br>\n",
    "e.g. <br>\n",
    "= 0 means output probabilities are *not* spread out i.e. Only 1 output token have a probability of 1.0 when the LLM is generating the output. This means it will always choose the same output during the decoding ending up with the same completion.<br>\n",
    "\\> 0, ~0 means output probabilities are a little spread out i.e. > 1 output tokens will have a probability of closer to 1.0, combining to 1.0. This means that the LLM will have a chance of sampling different output tokens.<br>\n",
    "= 2 means output probabilities are *most* spread out i.e. output tokens will have similar probabilities and the LLM will have similar chance of sampling from each token.\n",
    "\n",
    "`top_p` and `temperature` goes hand in hand. Having a really low temperature means there are less output tokens within the probability threshold.\n",
    "\n",
    "__Number of completions__ (relates to: self-consistency):\n",
    "+ `n_completions` - **this is the number of responses you're asking the LLM to generate.**<br>(increases cost but directly relates to the number of votes used by Self Consistency)\n",
    "\n",
    "__Penalties__:\n",
    "+ `presence_penalty` - **this forces the model to be more creative in their word choices per completion. i.e. it penalises words that the model have already said.** (-2.0 <= presence_penalty <= 2.0)<br>\n",
    "empirically, this have shown to output more tokens if increased.\n",
    "\n",
    "<br>\n",
    "ðŸ‘¼ Experiment with different sampling schemes and increase number of completions for more confidence in your votes (beware of costs!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8006703e-cb0f-45a5-a288-4eebe0b6de22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llm_experiments import SamplingScheme\n",
    "\n",
    "sampling_scheme = SamplingScheme(top_p=0.8, temperature=1, presence_penalty=0.0)\n",
    "n_completions = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ad013-f7f0-42c2-8f8c-f03f3d30a019",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Upload your Chain of Thoughts examples\n",
    "\n",
    "Chain of thoughts uses examples to help the LLM reason about your queries better by breaking it down into steps.\n",
    "\n",
    "The idea is to give it more context and prompt it to break down its reasoning process into steps.\n",
    "\n",
    "ðŸ‘¼ Experiment with different 'steps' for each example, I would recommend having a diverse set of reasons and be specific. <br> \n",
    "ðŸ‘¼ It's usually better with more examples but this also means it'll increase your API cost (see TikDollar later). Starting out with 3-5 examples per class should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9610eb5b-42a2-4f43-958b-7ea7ef0b2c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tempfile import mkdtemp\n",
    "from pathlib import Path\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "MARKDOWN = str\n",
    "\n",
    "uploader_data = dict(saved_path=None)\n",
    "\n",
    "def cb_save_to_file(fbytes: bytes, fname: str) -> MARKDOWN:\n",
    "    if fbytes is None or len(fbytes) <= 0 or fname is None: return \"\"\n",
    "    dir_ = Path(mkdtemp())\n",
    "    path = dir_.joinpath(fname)\n",
    "    with open(path, 'wb') as h:\n",
    "        h.write(finput.value)\n",
    "    uploader_data['toml'] = path\n",
    "    return f\"Received: **{fname}**\\t\\tSaved temporarily to **{uploader_data.get('toml')}**\"\n",
    "\n",
    "finput = pn.widgets.FileInput(accept='.toml')\n",
    "iobject = pn.bind(cb_save_to_file, finput, finput.param.filename)\n",
    "pn.Row(finput, pn.pane.Markdown(iobject))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bac09-53bf-4cdd-804a-f74f11b627a9",
   "metadata": {},
   "source": [
    "ðŸ‘¼ Currently supported models: text-davinci-003, text-davinci-002, text-curie-001, text-babbage-001, text-ada-001\n",
    "(In decreasing performance and cost.)<br>\n",
    "ðŸ‘¼ Newly Added: gpt-3.5-turbo at 10% of the cost of text-davinici-003 but similar performance!\n",
    "\n",
    "Note: If you enter the wrong model name, it'll tell you what's available in the error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab8662d-cac0-4ee9-8924-8b9161ed12cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llm_experiments import CoTSC\n",
    "\n",
    "assert uploader_data.get('toml'), \"Have you uploaded your Chain of Thoughts examples .toml configuration file?\"\n",
    "\n",
    "cotsc = CoTSC.from_toml(model='gpt-4',\n",
    "                        prompt_toml=uploader_data.get('toml'),\n",
    "                        sampling_scheme=sampling_scheme, \n",
    "                        n_completions=n_completions,\n",
    "                        shuffle_examples=True,\n",
    "                        shuffle_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9528ffac-71c0-48b4-ae2b-b6b02be27a3a",
   "metadata": {},
   "source": [
    "### Here's your prompt (Not sent to OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81387c2-b237-4ca3-9779-beeab83e29bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cotsc.dryrun(query=\"Canberra immunologist Carola Vinuesa who discovered a gene responsible for the autoimmune diseases lupus and diabetes.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67bd4f-f4a1-4ab6-9800-f8c0d93741b5",
   "metadata": {},
   "source": [
    "### Here's an example (Sent to OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6454085e-20d4-4052-b22d-41eec510f618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = cotsc.run(query=\"Canberra immunologist Carola Vinuesa who discovered a gene responsible for the autoimmune diseases lupus and diabetes.\")\n",
    "results  # raw output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08352823-a828-4900-8a04-1a7e1f39673b",
   "metadata": {},
   "source": [
    "We'll format the raw output a little bit by putting it in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b9777-d3bd-4c45-895e-9c9b6a2b701e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "\n",
    "pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66361306-65f2-4529-8b19-7b88fadefd52",
   "metadata": {},
   "source": [
    "### TikDollar\n",
    "**Tikdollar is created by SIH to track your OpenAI expense and cut if off at a specified threshold.<br>\n",
    "The code will run your calls right up until when your next call will exceed this threshold.**\n",
    "\n",
    "> OpenAI charges for both your input and output tokens. Each token can be thought of as a word in the normal sense but tokens for LLMs are actually *subwords*.\n",
    "\n",
    "For more information on subwords here's a cool video: https://www.youtube.com/watch?v=zHvTiHr506c\n",
    "\n",
    "**Now, we're going to bind TikDollar with our CoTSC calling function.**\n",
    "\n",
    "The parameters you need to care about:\n",
    "+ `cost_threshold`  - this is the cut-off USD. You'll need to define this. (e.g. 0.1, 1.0, 20)\n",
    "+ `raise_err` - when the cut-off will be exceeded in the next call, stop. (or print a message then continue if it's False)\n",
    "+ `verbose` - whether to print messages per call in terms of your spending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06feb66e-4b80-4452-95a2-41acabd9e790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llm_experiments.utils import TikDollar as td\n",
    "\n",
    "# âš ï¸ Caveat: When you rerun this cell, tikdollar is reset to 0!\n",
    "tikdollar = td.track(cotsc, cotsc._tikdollar_run, cost_threshold=0.1, raise_err=True, verbose=True)\n",
    "tikdollar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca295c7-3beb-4336-8f99-f4a540b9fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cotsc.run(query=\"Professor Lawford says the institute, based at the Greenslopes Private Hospital, in Brisbane's south, in conjunction with overseas collaborators, is analysing the DNA of Australian Vietnam veterans in a bid to better understand the causes of PTSD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277a86e-7d92-494d-9772-e694fe7eaf47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tikdollar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5bac93-8caa-446c-83fa-58ab24cfbaad",
   "metadata": {},
   "source": [
    "## 4. Batch Classification\n",
    "\n",
    "Now that you have **CoTSC** and **TikDollar** you're equipped to run a classification task on your list of sentences!\n",
    "\n",
    "At the end of this there'll be a link for you to click on to download all the generated results into an excel sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa1a8f-9fb3-4f7b-ba74-5f04da458f55",
   "metadata": {},
   "source": [
    "#### 1. Upload your dataset (requires: 'sentence' column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3edb2-5eb9-4225-8593-9da2f92aa320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_save_to_file(fbytes: bytes, fname: str) -> MARKDOWN:\n",
    "    if fbytes is None or len(fbytes) <= 0 or fname is None: return \"\"\n",
    "    dir_ = Path(mkdtemp())\n",
    "    path = dir_.joinpath(fname)\n",
    "    with open(path, 'wb') as h:\n",
    "        h.write(finput.value)\n",
    "    uploader_data['dataset'] = path\n",
    "    return f\"Received: **{fname}**\\t\\tSaved temporarily to **{uploader_data.get('dataset')}**\"\n",
    "\n",
    "finput = pn.widgets.FileInput(accept='.xlsx')\n",
    "iobject = pn.bind(cb_save_to_file, finput, finput.param.filename)\n",
    "pn.Row(finput, pn.pane.Markdown(iobject))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f59ab-b379-4139-9fb7-74c65165e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "assert uploader_data.get('dataset'), \"Did you upload your dataset?\"\n",
    "df = pd.read_excel(uploader_data.get('dataset'))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c961a5-7fa5-4829-9247-a4fc61032354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot class distribution\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "fig = make_subplots(rows=1, cols=4)\n",
    "\n",
    "fig.add_trace(go.Histogram(x=df['det']), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=df['hom']), row=1, col=2)\n",
    "fig.add_trace(go.Histogram(x=df['se']), row=1, col=3)\n",
    "fig.add_trace(go.Histogram(x=df['nat']), row=1, col=4)\n",
    "\n",
    "fig.update_xaxes(title_text=\"determinism\", dtick=1, row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"homogeneity\", dtick=1, row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"specific aetiology\", dtick=1, row=1, col=3)\n",
    "fig.update_xaxes(title_text=\"naturalism\", dtick=1, row=1, col=4)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Count\", range=[0, len(df)])\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(height=300, width=1400, title_text=\"Distribution of Classes\", showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703cb415-29d4-4bb6-88b2-936660ce3131",
   "metadata": {},
   "source": [
    "#### 2. Set up TikDollar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e435f1a0-643d-4220-887d-9282f65f86ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup TikDollar\n",
    "# copy of prior cell for task separation and easy access.\n",
    "from llm_experiments.utils import TikDollar as td\n",
    "\n",
    "# âš ï¸ Caveat: When you rerun this cell, tikdollar is reset to 0!\n",
    "tikdollar = td.track(cotsc, cotsc._tikdollar_run, cost_threshold=20, raise_err=True, verbose=True)\n",
    "tikdollar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6577da-b604-4747-8dbf-32f00ec529a6",
   "metadata": {},
   "source": [
    "#### 3. Run classification on uploaded dataset\n",
    "\n",
    "ðŸ§‘â€ðŸ’» If you find the TikDollar messages cluttering your screen, set `verbose=False` in the previous cell, run the cell and then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd145a82-d21e-4c6e-9de4-51e60be85462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_experiments.utils.tikdollar import CostThresholdReachedException\n",
    "from llm_experiments.cot import CoTDataLeakException\n",
    "from collections import namedtuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ROW = namedtuple('ROW', ['query', 'clazz', 'votes', 'steps', 'determinism', 'specific_aetiology', 'naturalness', 'homogeneity', 'is_biased', 'completions'])\n",
    "\n",
    "Rows = list()\n",
    "dleak_counter = 0\n",
    "for i, row in tqdm(enumerate(df.itertuples()), total=len(df)):\n",
    "    query = row.sentence\n",
    "    det, se, nat, hom, pos = row.det, row.se, row.nat, row.hom, row.pos\n",
    "    try:\n",
    "        results = cotsc.run(query=query)\n",
    "        for clazz, clz_res in results.items():\n",
    "            Row = ROW(query=query, clazz=clazz, votes=clz_res.get('votes'), steps=clz_res.get('steps'), \n",
    "                      determinism=det, specific_aetiology=se, naturalness=nat, homogeneity=hom, is_biased=pos, completions=clz_res.get('completions'))\n",
    "            Rows.append(Row)\n",
    "    except CostThresholdReachedException as ctre:\n",
    "        print(ctre)\n",
    "        print(f\"Number of queries sent: {i}.\")\n",
    "        break\n",
    "    except CoTDataLeakException as cotdle:\n",
    "        print(cotdle)\n",
    "        print(\"Data leak detected. Skipped.\")\n",
    "        dleak_counter += 1\n",
    "        continue\n",
    "\n",
    "print(f\"Number of examples leaked: {dleak_counter}\")\n",
    "results_df = pd.DataFrame(Rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05595b-d4c8-4824-a924-7e8a711820d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(Rows)\n",
    "len(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd9507-8edc-4f76-be52-f8600adefa93",
   "metadata": {},
   "source": [
    "#### 4. Analyse classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2235d923-d630-4f48-84ef-3fe1d8ab5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ce7fd-feeb-4ae3-a12b-6f55f6335cab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df.clazz.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d13ad-da82-4f67-8f73-da87bfd58598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "clazz_to_idx = {\n",
    "    \"determinism\": 0,\n",
    "    \"specific_aetiology\": 1,\n",
    "    \"naturalness\": 2,\n",
    "    \"homogeneity\": 3,\n",
    "}\n",
    "\n",
    "def clazz_to_row(clazz: str) -> list[int]:\n",
    "    # clean up llm output\n",
    "    try:\n",
    "        comma_idx = clazz.index(',')\n",
    "        clazz = clazz[:comma_idx]\n",
    "    except:\n",
    "        pass\n",
    "    clazz = clazz.strip()\n",
    "    row = [0, 0, 0, 0]\n",
    "    if clazz == 'not_biased': \n",
    "        return row\n",
    "    elif clazz_to_idx.get(clazz, None) is None:\n",
    "        print(f\"{clazz} is not one of {', '.join(clazz_to_idx.keys())}. Continued as 'not_biased'\", file=sys.stderr)\n",
    "        return row\n",
    "    else:\n",
    "        row[clazz_to_idx.get(clazz)] = 1\n",
    "        return row\n",
    "\n",
    "preds, targets = list(), list()\n",
    "for query, group in results_df.groupby(by='query'):\n",
    "    best_idx = group.loc[:, 'votes'].idxmax()\n",
    "    best = group.loc[best_idx]\n",
    "    prediction = clazz_to_row(best.clazz)\n",
    "    target = group[['determinism', 'specific_aetiology', 'naturalness', 'homogeneity']].values[0]\n",
    "    preds.append(prediction)\n",
    "    targets.append(target)\n",
    "\n",
    "preds, targets = np.array(preds), np.array(targets)\n",
    "assert preds.shape == targets.shape, \"Mismatched shape between prediction and targets. This should not happen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8127352-540e-449a-a701-9f0b207dfc83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import sys\n",
    "\n",
    "def report(clazzes, targets, preds, file=sys.stdout):\n",
    "    for i in range(len(clazzes)):\n",
    "        print(f\"=== {clazzes[i]} ===\", file=file)\n",
    "        print(classification_report(y_true=targets[:, i], y_pred=preds[:, i]), file=file)\n",
    "        print(\"\\n\", file=file)\n",
    "    \n",
    "clazzes = list(clazz_to_idx.keys())\n",
    "report(clazzes, targets, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d3c7fd-746d-4df0-9b1b-f79a88d1d860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cm(clazzes, targets, preds):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    for i in range(len(clazzes)):\n",
    "        y_true, y_pred = targets[:, i], preds[:, i]\n",
    "        clazz = clazzes[i]\n",
    "        if sum(y_true) == 0:\n",
    "            labels = ['neutral']\n",
    "        else:\n",
    "            labels = ['neutral', clazz]\n",
    "\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix(y_true=y_true, y_pred=y_pred), display_labels=labels)\n",
    "        ax = axes[i//2, i%2]\n",
    "        ax.set_title(f\"{clazz}\")\n",
    "        disp.plot(ax=axes[i//2, i%2])\n",
    "    return fig\n",
    "fig = cm(clazzes, targets, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec0f73-e0a0-4bbd-b819-7d05330c3dba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert to single-target-multi-class confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "def to_labels(onehot):\n",
    "    \"\"\" This converts the [det, se, hom, nat] to [neutral, det, se, hom, nat] \"\"\"\n",
    "    num_classes_plus_neutral = onehot.shape[1] + 1\n",
    "    labels = np.zeros((onehot.shape[0], num_classes_plus_neutral))\n",
    "    for idx in range(len(onehot)):\n",
    "        if np.sum(onehot[idx]) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            clz_idx = np.argmax(onehot[idx])\n",
    "            clz_idx = clz_idx + 1\n",
    "            labels[idx][clz_idx] = 1\n",
    "    return labels\n",
    "\n",
    "# dynamic display_labels (dynamic to the samples e.g. SE only, or SE,DET only where target is a subset of all biases)\n",
    "all_labels=['neutral'] + clazzes\n",
    "y_true=np.argmax(to_labels(targets), axis=1)\n",
    "uniques = np.unique(y_true)\n",
    "display_labels = ['neutral']\n",
    "for u in uniques:\n",
    "    display_labels += [all_labels[u]]\n",
    "\n",
    "fig_st, axes = plt.subplots(figsize=(8, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(\n",
    "    y_true=y_true,\n",
    "    y_pred=np.argmax(to_labels(preds), axis=1)\n",
    "), display_labels=display_labels)\n",
    "disp.plot(ax=axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662fdda-fa0e-4422-bee8-04122ae2bd0c",
   "metadata": {},
   "source": [
    "## 5. Download Results\n",
    "1. Full COTSC outputs. (cotsc-outputs.xlsx)\n",
    "2. Classification results. (cotsc-results.txt)\n",
    "3. COTSC model configuration (cotsc-config.json)\n",
    "4. COTSC prompt toml (your uploaded toml file)\n",
    "5. Dataset used for classification.\n",
    "\n",
    "ðŸ‘¼ These are all packaged into `cotsc-results-{timestamp}.zip`. Which you'll be able to click and download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630c911-1836-494a-9673-f024bcb2fab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the temporary results directory\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "dir_: Path = Path('./.causation-cotsc-results')   # note: hidden folder.\n",
    "if dir_.exists(): shutil.rmtree(dir_)\n",
    "os.makedirs(dir_, exist_ok=True)\n",
    "assert dir_.exists(), \"Temporary directory did not get created.\"\n",
    "f\"Temporary directory: {dir_}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118629dd-565c-457b-85fc-01b183486838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reformat df for readability.\n",
    "formatted_results_df = results_df.copy()\n",
    "formatted_results_df['steps'] = formatted_results_df['steps'].apply(lambda steps: \"\\n\".join((f\"{i+1}. {s}\" for i, s in enumerate(steps))))\n",
    "formatted_results_df['completions'] = formatted_results_df['completions'].apply(lambda completions: \"\\n\\n\".join((c for c in completions)))\n",
    "formatted_results_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8914f-37f1-44a3-967d-98569fd698fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# classification outputs\n",
    "path = dir_.joinpath('cotsc-outputs.xlsx')\n",
    "formatted_results_df.to_excel(path, index=False)\n",
    "\n",
    "assert path.exists(), f\"Failed to save to {path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c7c6d-4242-4ab3-b8bf-b3567e8dc9b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# classification evaluation results\n",
    "path = dir_.joinpath('cotsc-results.txt')\n",
    "with open(path, 'w') as f:\n",
    "    clazzes = list(clazz_to_idx.keys())\n",
    "    num_clazzes = preds.shape[1]\n",
    "    report(clazzes, targets, preds, file=f)\n",
    "    \n",
    "assert path.exists(), f\"Failed to save to {path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f18ed1-5673-4597-b711-0677aa62cc3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# classification evaluation confusion matrix\n",
    "path = dir_.joinpath('cotsc-confusion-matrix.png')\n",
    "path_st = dir_.joinpath('cotsc-confusion-matrix-st.png')\n",
    "assert fig, \"Did you run the confusion matrix cell earlier?\"\n",
    "assert fig_st, \"Did you run the confusion matrix (single target) cell earlier?\"\n",
    "_ = fig.savefig(path)\n",
    "_ = fig_st.savefig(path_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63c9014-5ddc-4c5e-9c1a-7e37057a50f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cotsc configuration\n",
    "import srsly\n",
    "\n",
    "path = dir_.joinpath('cotsc-config.json')\n",
    "cotsc_config = {\n",
    "    'sampling_scheme': sampling_scheme.openai(),\n",
    "    'n_completions': cotsc.n_completions,\n",
    "    'model': cotsc.model,\n",
    "    'classes': cotsc.classes,\n",
    "}\n",
    "srsly.write_json(path, cotsc_config)\n",
    "assert path.exists(), f\"Failed to save to {path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff059ac6-449d-43d7-911f-6ec4a1690867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of names of files to be added to the zip\n",
    "file_names = [uploader_data['toml'], uploader_data['dataset']]  # toml & dataset\n",
    "file_names += list(dir_.glob('*'))\n",
    "\n",
    "[f.name for f in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f314a54e-0f4e-4aa4-9694-fb0a89ab3f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open a zip file in write mode\n",
    "# zip results and causation config\n",
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "now = datetime.now().strftime(format=\"%Y-%m-%d_%H-%M-%S\")\n",
    "zfname = Path(f'cotsc-results_{now}.zip')\n",
    "with zipfile.ZipFile(zfname, 'w') as zipf:\n",
    "    # Loop through the list of files\n",
    "    for file_name in file_names:\n",
    "        # Add each file to the zip\n",
    "        zipf.write(file_name, arcname=os.path.basename(file_name))\n",
    "print(f\"Saved to {zfname}\")\n",
    "\n",
    "# todo: download link for the zip.\n",
    "pn.widgets.FileDownload(file=str(zfname), filename=zfname.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f3521-82a1-4d25-83a3-321ae527312e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

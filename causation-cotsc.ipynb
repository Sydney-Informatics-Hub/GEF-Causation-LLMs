{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06444d10-e246-4eb6-b2fc-ba294bd6748f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Causation (Chain of Thoughts Self Consistency)\n",
    "\n",
    "Chain of Thoughts is a prompting technique that use examples containing steps to assist with the reasoning ability of a large language model (llm).\n",
    "\n",
    "Self consistency is a layer that adds on top that leverage the probabilistic outputs from LLMs and take the majority vote as the final answer.\n",
    "\n",
    "This notebook is an implementation of this for the GEF causation project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e8db91-60c6-4119-b94a-805ccda02216",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OpenAI Privacy Policy\n",
    "This notebook uses OpenAI's API, meaning that your data will be sent to the OpenAI servers.\n",
    "\n",
    "For concerns about how your data will be handled, please read through the Privacy Policy [here](https://openai.com/policies/api-data-usage-policies)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42c79c-3385-46bc-9d5f-c966da6c7305",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Paper: Chain of Thoughts (expand to read)\n",
    "Chain of Thought (CoT): https://arxiv.org/abs/2201.11903<br>\n",
    "CoT Self Consistency (CoT-SC): https://arxiv.org/abs/2203.11171"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc72e9f-ac88-4d34-9929-6bd73cdbcb3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "display(IFrame(src='https://arxiv.org/pdf/2201.11903.pdf', width=1600, height=700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9842cdf-2083-4177-8708-1c5cb4430787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "# CoT - Self Consistency\n",
    "display(IFrame(src='https://arxiv.org/pdf/2203.11171.pdf', width=1600, height=700))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a675e4-bda1-48ee-9c60-575f51eab885",
   "metadata": {},
   "source": [
    "## Overview\n",
    "1. Supply your OpenAI API Key\n",
    "2. Upload your file with the Chain of Thoughts examples.\n",
    "3. Choose a sampling scheme and number of completions\n",
    "4. Enter a query (i.e. the sentence) and run it for a classification.\n",
    "5. Batch classification using your dataset. **requires: sentence, det, se, hom, nat columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed9d9e9-3880-4194-8c48-0d14a3f8fb1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Enter your OpenAI API Key.\n",
    "\n",
    "To create an API Key, please go to: https://platform.openai.com/account/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5493e91-ee37-43d3-85f5-7ec2065abe97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from causation.utils import openai_apikey_input\n",
    "\n",
    "openai_apikey_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ad013-f7f0-42c2-8f8c-f03f3d30a019",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Upload your Chain of Thoughts examples\n",
    "\n",
    "Chain of thoughts uses examples to help the LLM reason about your queries better by breaking it down into steps.\n",
    "\n",
    "The idea is to give it more context and prompt it to break down its reasoning process into steps.\n",
    "\n",
    "👼 Experiment with different 'steps' for each example, I would recommend having a diverse set of reasons and be specific. <br> \n",
    "👼 It's usually better with more examples but this also means it'll increase your API cost (see TikDollar later). Starting out with 3-5 examples per class should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01a6df-100d-4031-b172-2d47a3204134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from causation.utils import fileuploader \n",
    "\n",
    "finput, exemplars = fileuploader('.toml')\n",
    "finput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ecf0df-ae19-4da4-bb68-b251474154de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoding issue - 27.Sep.23 - this strips away the whitespace/end-of-line character at the end of each line.\n",
    "# it's okay to always run this cell with or without the encoding issue.\n",
    "!sed 's/\" $/\"/g' \"{exemplars.get('data').absolute()}\" > \"{exemplars.get('data').absolute()}.formatted\"\n",
    "!mv \"{exemplars.get('data').absolute()}.formatted\" \"{exemplars.get('data').absolute()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d12bc15-7be6-4fcb-9203-37903525c6ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert exemplars.get('data'), \"Did you upload your CoT examples in the previous cell?\"\n",
    "\n",
    "from llm_experiments import CoT\n",
    "\n",
    "cot = CoT.from_toml(exemplars.get('data'))\n",
    "cot.shuffle_examples()                                         # improves result\n",
    "f\"These CoT example class distributions: {cot.class_dist()}\"   # try to keep this balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bac09-53bf-4cdd-804a-f74f11b627a9",
   "metadata": {},
   "source": [
    "👼 Currently supported models: text-davinci-003, text-davinci-002, text-curie-001, text-babbage-001, text-ada-001\n",
    "(In decreasing performance and cost.)<br>\n",
    "👼 Newly Added: gpt-3.5-turbo at 10% of the cost of text-davinici-003 but similar performance!\n",
    "\n",
    "Note: If you enter the wrong model name, it'll tell you what's available in the error message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf3ce8-ab78-4460-91a2-1b48c0a3423e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Optional] Sample only a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ac52f-7546-4de4-a7fa-d8a4ee134bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [Optional] - randomly sample X CoT examples to reduce input tokens.\n",
    "cot.sample(method='random', n=len(cot.examples))\n",
    "cot.class_dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d4eda-3939-4f8b-869c-065383e2b2e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Choose a sampling scheme and the number of completions.\n",
    "\n",
    "__Sampling Scheme__:\n",
    "+ `top_p` - **the range of output words within a probability threshold.** (0 < top_p <= 1.0)<br>\n",
    "e.g. <br>\n",
    "= 0.3 means sample only from outputs that make up the top 30% of the probabilities.\n",
    "\n",
    "+ `temperature` - **the higher the temperature the more spread out the probabilities are across the output words.** (0 <= temperature <= 2) <br>\n",
    "e.g. <br>\n",
    "= 0 means output probabilities are *not* spread out i.e. Only 1 output token have a probability of 1.0 when the LLM is generating the output. This means it will always choose the same output during the decoding ending up with the same completion.<br>\n",
    "\\> 0, ~0 means output probabilities are a little spread out i.e. > 1 output tokens will have a probability of closer to 1.0, combining to 1.0. This means that the LLM will have a chance of sampling different output tokens.<br>\n",
    "= 2 means output probabilities are *most* spread out i.e. output tokens will have similar probabilities and the LLM will have similar chance of sampling from each token.\n",
    "\n",
    "`top_p` and `temperature` goes hand in hand. Having a really low temperature means there are less output tokens within the probability threshold.\n",
    "\n",
    "__Number of completions__ (relates to: self-consistency):\n",
    "+ `n_completions` - **this is the number of responses you're asking the LLM to generate.**<br>(increases cost but directly relates to the number of votes used by Self Consistency)\n",
    "\n",
    "__Penalties__:\n",
    "+ `presence_penalty` - **this forces the model to be more creative in their word choices per completion. i.e. it penalises words that the model have already said.** (-2.0 <= presence_penalty <= 2.0)<br>\n",
    "empirically, this have shown to output more tokens if increased.\n",
    "\n",
    "<br>\n",
    "👼 Experiment with different sampling schemes and increase number of completions for more confidence in your votes (beware of costs!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8006703e-cb0f-45a5-a288-4eebe0b6de22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llm_experiments import SamplingScheme\n",
    "\n",
    "sampling_scheme = SamplingScheme(top_p=0.8, temperature=1, presence_penalty=0.0)\n",
    "n_completions = 3\n",
    "\n",
    "assert n_completions > 1, \"For the model to generate > 1 possibilities needed for self-consistency, n_completions must be > 1.\"\n",
    "assert sampling_scheme.temperature > 0, \"For the model to generate > 1 possibilities needed for self-consistency, temperature must be > 0.\"\n",
    "sampling_scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d1a3e3-fc44-4806-814a-0ed5208a2e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llm_experiments import CoTSC\n",
    "\n",
    "cotsc = CoTSC.from_cot(model='gpt-3.5-turbo',  # for a larger context window (from 4k -> 16k tokens) replace with 'gpt-3.5-turbo-16k'\n",
    "                       cot=cot,\n",
    "                       sampling_scheme=sampling_scheme,\n",
    "                       n_completions=n_completions)\n",
    "f\"{cotsc.model}   'temperature': {cotsc.llm.temperature}, {str(cotsc.llm.model_kwargs).lstrip('{').rstrip('}')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81387c2-b237-4ca3-9779-beeab83e29bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt, num_tokens = cotsc.dryrun(query=\"Canberra immunologist Carola Vinuesa who discovered a gene responsible for the autoimmune diseases lupus and diabetes.\")\n",
    "print(prompt)\n",
    "f\"Number of tokens in the above prompt: {num_tokens}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67bd4f-f4a1-4ab6-9800-f8c0d93741b5",
   "metadata": {},
   "source": [
    "### Here's an example (Sent to OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6454085e-20d4-4052-b22d-41eec510f618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "\n",
    "results = cotsc.run(query=\"Canberra immunologist Carola Vinuesa who discovered a gene responsible for the autoimmune diseases lupus and diabetes.\")\n",
    "pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66361306-65f2-4529-8b19-7b88fadefd52",
   "metadata": {},
   "source": [
    "### TikDollar\n",
    "**Tikdollar is created by SIH to track your OpenAI expense and cut if off at a specified threshold.<br>\n",
    "The code will run your calls right up until when your next call will exceed this threshold.**\n",
    "\n",
    "> OpenAI charges for both your input and output tokens. Each token can be thought of as a word in the normal sense but tokens for LLMs are actually *subwords*.\n",
    "\n",
    "For more information on subwords here's a cool video: https://www.youtube.com/watch?v=zHvTiHr506c\n",
    "\n",
    "**Now, we're going to bind TikDollar with our CoTSC calling function.**\n",
    "\n",
    "The parameters you need to care about:\n",
    "+ `cost_threshold`  - this is the cut-off USD. You'll need to define this. (e.g. 0.1, 1.0, 20)\n",
    "+ `raise_err` - when the cut-off will be exceeded in the next call, stop. (or print a message then continue if it's False)\n",
    "+ `verbose` - whether to print messages per call in terms of your spending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06feb66e-4b80-4452-95a2-41acabd9e790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llm_experiments.utils import TikDollar as td\n",
    "\n",
    "# ⚠️ Caveat: When you rerun this cell, tikdollar is reset to 0!\n",
    "tikdollar = td.track(cotsc, cotsc._tikdollar_run, cost_threshold=0.1, raise_err=True, verbose=True)\n",
    "tikdollar  # starts out with zero cost accumulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca295c7-3beb-4336-8f99-f4a540b9fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cotsc.run(query=\"Professor Lawford says the institute, based at the Greenslopes Private Hospital, in Brisbane's south, in conjunction with overseas collaborators, is analysing the DNA of Australian Vietnam veterans in a bid to better understand the causes of PTSD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277a86e-7d92-494d-9772-e694fe7eaf47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tikdollar # cost after you call the API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5bac93-8caa-446c-83fa-58ab24cfbaad",
   "metadata": {},
   "source": [
    "## 4. Batch Classification\n",
    "\n",
    "Now that you have **CoTSC** and **TikDollar** you're equipped to run a classification task on your list of sentences!\n",
    "\n",
    "At the end of this there'll be a link for you to click on to download all the generated results into an excel sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa1a8f-9fb3-4f7b-ba74-5f04da458f55",
   "metadata": {},
   "source": [
    "#### 1. Upload your dataset (requires: 'sentence' column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e388aa6a-4a64-446d-b45c-59f094f86285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from causation.utils import fileuploader\n",
    "\n",
    "finput, dataset = fileuploader('.xlsx')\n",
    "finput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f59ab-b379-4139-9fb7-74c65165e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "assert dataset.get('data'), \"Did you upload your dataset?\"\n",
    "df = pd.read_excel(dataset.get('data'))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b72657-f759-44a6-9b5f-2f9d6447c906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# validate uploaded dataset\n",
    "required_columns = ['sentence', 'det', 'se', 'hom', 'nat']\n",
    "for req_col in required_columns:\n",
    "    assert req_col in df.columns, f\"Missing column: '{req_col}' in {list(df.columns)}\"\n",
    "f\"All required columns found: {required_columns}. Please continue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e1873b-d9f2-4aa4-9601-3d4ea5334719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'pos' in df.columns:\n",
    "    import sys\n",
    "    print(\"- [warn] Found 'pos' column in your dataset, \"\n",
    "          \"these values will be recomputed to represent whether any bias exist. \"\n",
    "          \"Do not worry about this warning message. \"\n",
    "          \"(This will not overwrite your existing excel file.)\", \n",
    "          file=sys.stderr)\n",
    "df['pos'] = df.loc[:, ['det', 'se', 'hom', 'nat']].sum(axis=1).apply(lambda sum_: min(sum_, 1))\n",
    "assert 'pos' in df.columns, \"Missing 'pos' column. This should not happen after it's been added\"\n",
    "f\"'pos' column now represents whether any 'bias' exist. Please continue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f1d7f-74a7-4b84-b046-cb45a9d642f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check and fill NaN values with 0 for label columns\n",
    "label_columns = ['det', 'se', 'hom', 'nat']\n",
    "hasnans = df.loc[:, label_columns].isnull().values.any()\n",
    "if hasnans:\n",
    "    import sys\n",
    "    print(\"- [warn] Found NaN values in the 'det', 'se','hom', 'nat' columns. \"\n",
    "          \"These values will be replaced with 0s.\",\n",
    "          file=sys.stderr)\n",
    "    for col in label_columns:\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "hasnans = df.loc[:, label_columns].isnull().values.any()\n",
    "assert not hasnans, f\"There are still NaN values in the label columns: {label_columns}. This should not happen.\"\n",
    "f\"Label columns {label_columns} checked to not have any NaN values. Please continue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c961a5-7fa5-4829-9247-a4fc61032354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot class distribution\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "fig = make_subplots(rows=1, cols=4)\n",
    "\n",
    "fig.add_trace(go.Histogram(x=df['det']), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=df['hom']), row=1, col=2)\n",
    "fig.add_trace(go.Histogram(x=df['se']), row=1, col=3)\n",
    "fig.add_trace(go.Histogram(x=df['nat']), row=1, col=4)\n",
    "\n",
    "fig.update_xaxes(title_text=\"determinism\", dtick=1, row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"homogeneity\", dtick=1, row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"specific aetiology\", dtick=1, row=1, col=3)\n",
    "fig.update_xaxes(title_text=\"naturalism\", dtick=1, row=1, col=4)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Count\", range=[0, len(df)])\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(height=300, width=1400, title_text=\"Distribution of Classes\", showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703cb415-29d4-4bb6-88b2-936660ce3131",
   "metadata": {},
   "source": [
    "#### 2. Set up TikDollar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e435f1a0-643d-4220-887d-9282f65f86ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup TikDollar\n",
    "# copy of prior cell for task separation and easy access.\n",
    "from llm_experiments.utils import TikDollar as td\n",
    "\n",
    "# ⚠️ Caveat: When you rerun this cell, tikdollar is reset to 0!\n",
    "tikdollar = td.track(cotsc, cotsc._tikdollar_run, cost_threshold=20, raise_err=True, verbose=False)\n",
    "tikdollar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6577da-b604-4747-8dbf-32f00ec529a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3. Run classification on uploaded dataset\n",
    "\n",
    "🧑‍💻 If you find the TikDollar messages cluttering your screen, set `verbose=False` in the previous cell, run the cell and then run the cell below.\n",
    "\n",
    "**Data leakage**:<br>\n",
    "We expect the examples in your dataset to not be one of your CoT examples. When it is, the data has leaked as the model will just answer what you've provided leading to a biased evaluation. These examples are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd145a82-d21e-4c6e-9de4-51e60be85462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_experiments.utils.tikdollar import CostThresholdReachedException\n",
    "from llm_experiments.cot.cot import CoTDataLeakException\n",
    "from collections import namedtuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ROW = namedtuple('ROW', ['query', 'clazz', 'votes', 'steps', 'determinism', 'specific_aetiology', 'naturalness', 'homogeneity', 'is_biased', 'completions'])\n",
    "\n",
    "Rows = list()\n",
    "dleak_counter = 0\n",
    "for i, row in tqdm(enumerate(df.itertuples()), total=len(df)):\n",
    "    query = row.sentence\n",
    "    det, se, nat, hom, pos = row.det, row.se, row.nat, row.hom, row.pos\n",
    "    try:\n",
    "        results = cotsc.run(query=query)\n",
    "        for clazz, clz_res in results.items():\n",
    "            Row = ROW(query=query, clazz=clazz, votes=clz_res.get('votes'), steps=clz_res.get('steps'), \n",
    "                      determinism=det, specific_aetiology=se, naturalness=nat, homogeneity=hom, is_biased=pos, completions=clz_res.get('completions'))\n",
    "            Rows.append(Row)\n",
    "    except CostThresholdReachedException as ctre:\n",
    "        print(ctre)\n",
    "        print(f\"Number of queries sent: {i}.\")\n",
    "        break\n",
    "    except CoTDataLeakException as cotdle:\n",
    "        print(cotdle)\n",
    "        print(\"Data leak detected. Skipped.\")\n",
    "        dleak_counter += 1\n",
    "        continue\n",
    "\n",
    "print(f\"Number of examples leaked: {dleak_counter}\")\n",
    "results_df = pd.DataFrame(Rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5cade6-ec80-48a4-a6c2-a7602b166fa0",
   "metadata": {},
   "source": [
    "Data leakage is when a 'query' is matched exactly with one of the examples given in your CoT examples (i.e. the training examples in the .toml file you've uploaded earlier). These queries are not sent to OpenAI as the model will give the exact answer you've given in your example which is not representative of the model's accuracy, and I suppose it also doesn't require a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05595b-d4c8-4824-a924-7e8a711820d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(Rows)\n",
    "f\"Obtained {len(results_df)} results.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71e3f2d-6df7-466d-9ecd-69475d02e640",
   "metadata": {},
   "source": [
    "The above number of results may not match the number of queries.\n",
    "This is because of *self-consistency* (see `n_completions` argument in section 3) and model may generate multiple answers for one query. The majority vote is taken as the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd9507-8edc-4f76-be52-f8600adefa93",
   "metadata": {},
   "source": [
    "#### 4. Analyse classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2235d923-d630-4f48-84ef-3fe1d8ab5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a quick look at the results\n",
    "results_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ce7fd-feeb-4ae3-a12b-6f55f6335cab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df.clazz.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d13ad-da82-4f67-8f73-da87bfd58598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "clazz_to_idx = {\n",
    "    \"determinism\": 0,\n",
    "    \"specific_aetiology\": 1,\n",
    "    \"naturalness\": 2,\n",
    "    \"homogeneity\": 3,\n",
    "}\n",
    "\n",
    "def clazz_to_onehot(clazz: str) -> list[int]:\n",
    "    # clean up llm output\n",
    "    try:\n",
    "        comma_idx = clazz.index(',')\n",
    "        clazz = clazz[:comma_idx]\n",
    "    except:\n",
    "        pass\n",
    "    clazz = clazz.strip()\n",
    "    row = [0, 0, 0, 0]\n",
    "    if clazz in ('not_biased', 'neutral'): \n",
    "        return row\n",
    "    elif clazz_to_idx.get(clazz, None) is None:\n",
    "        print(f\"{clazz} is not one of {', '.join(clazz_to_idx.keys())}. Continued as 'neutral'\", file=sys.stderr)\n",
    "        return row\n",
    "    else:\n",
    "        row[clazz_to_idx.get(clazz)] = 1\n",
    "        return row\n",
    "\n",
    "preds, targets = list(), list()\n",
    "for query, group in results_df.groupby(by='query'):\n",
    "    best_idx = group.loc[:, 'votes'].idxmax()   # ties are randomly selected here.\n",
    "    best = group.loc[best_idx]\n",
    "    prediction = clazz_to_onehot(best.clazz)\n",
    "    target = group[['determinism', 'specific_aetiology', 'naturalness', 'homogeneity']].values[0]\n",
    "    preds.append(prediction)\n",
    "    targets.append(target)\n",
    "\n",
    "preds, targets = np.array(preds), np.array(targets)\n",
    "assert preds.shape == targets.shape, \"Mismatched shape between prediction and targets. This should not happen.\"\n",
    "\"Computed values to generate classification report. Please continue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8127352-540e-449a-a701-9f0b207dfc83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# classification report\n",
    "from sklearn.metrics import classification_report\n",
    "import sys\n",
    "\n",
    "def report(clazzes, targets, preds, file=sys.stdout):\n",
    "    for i in range(len(clazzes)):\n",
    "        print(f\"=== {clazzes[i]} ===\", file=file)\n",
    "        print(classification_report(y_true=targets[:, i], y_pred=preds[:, i], zero_division=0), file=file)\n",
    "        print(\"\\n\", file=file)\n",
    "    \n",
    "clazzes = list(clazz_to_idx.keys())\n",
    "report(clazzes, targets, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d3c7fd-746d-4df0-9b1b-f79a88d1d860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cm(clazzes, targets, preds):\n",
    "    num_rows, num_cols = 2, 2\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(5*num_rows, 5*num_rows))\n",
    "    for i in range(len(clazzes)):\n",
    "        clazz = clazzes[i]\n",
    "        labels = ['neutral', clazz]\n",
    "        y_true, y_pred = list(map(lambda x: labels[int(x)], targets[:, i])), list(map(lambda x: labels[int(x)], preds[:, i]))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix(y_true=y_true, y_pred=y_pred, labels=labels), display_labels=labels)\n",
    "        r, c = i//2, i%2\n",
    "        ax = axes[r, c]\n",
    "        ax.set_title(f\"{clazz}\")\n",
    "        disp.plot(ax=axes[r, c])\n",
    "    return fig\n",
    "fig = cm(clazzes, targets, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec0f73-e0a0-4bbd-b819-7d05330c3dba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# single-target-multi-class confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "def to_labels(onehot):\n",
    "    \"\"\" This converts the [det, se, hom, nat] to [neutral, det, se, hom, nat] \"\"\"\n",
    "    num_classes_plus_neutral = onehot.shape[1] + 1\n",
    "    labels = np.zeros((onehot.shape[0], num_classes_plus_neutral))\n",
    "    for idx in range(len(onehot)):\n",
    "        if np.sum(onehot[idx]) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            clz_idx = np.argmax(onehot[idx])\n",
    "            clz_idx = clz_idx + 1\n",
    "            labels[idx][clz_idx] = 1\n",
    "    return labels\n",
    "\n",
    "# dynamic display_labels (dynamic to the samples e.g. SE only, or SE,DET only where target is a subset of all biases)\n",
    "all_labels = ['neutral'] + list(map(lambda name: name[:3] if len(name.split(\"_\")) == 1 else \"_\".join([chunk[:2] for chunk in name.split(\"_\")]), clazzes))\n",
    "y_true=np.argmax(to_labels(targets), axis=1)\n",
    "y_pred=np.argmax(to_labels(preds), axis=1)\n",
    "\n",
    "y_true = list(map(lambda i: all_labels[i], y_true))\n",
    "y_pred = list(map(lambda i: all_labels[i], y_pred))\n",
    "\n",
    "fig_st, axes = plt.subplots(figsize=(8, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    "    labels=all_labels,\n",
    "), display_labels=all_labels)\n",
    "disp.plot(ax=axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662fdda-fa0e-4422-bee8-04122ae2bd0c",
   "metadata": {},
   "source": [
    "## 5. Download Results\n",
    "1. Full COTSC outputs. (cotsc-outputs.xlsx)\n",
    "2. Classification results. (cotsc-results.txt)\n",
    "3. COTSC model configuration (cotsc-config.json)\n",
    "4. COTSC prompt toml (your uploaded toml file)\n",
    "5. Dataset used for classification.\n",
    "\n",
    "👼 Please run through all the cells. All results are packaged into `cotsc-results-{timestamp}.zip` at the final cell and a download link will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630c911-1836-494a-9673-f024bcb2fab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the temporary results directory\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "dir_: Path = Path('./.causation-cotsc-results')   # note: hidden folder.\n",
    "if dir_.exists(): shutil.rmtree(dir_)\n",
    "os.makedirs(dir_, exist_ok=True)\n",
    "assert dir_.exists(), \"Temporary directory did not get created.\"\n",
    "f\"Results will be saved to this temporary directory: {dir_}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad73864-a6e6-4db1-b09b-a65622a830bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reformat df for readability. (Adds veracity column)\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "Row = namedtuple('Row', ['query', 'majority_pred', 'veracity', 'votes', 'steps_per_vote', 'minority_preds', 'det', 'se', 'nat', 'hom', 'neutral', 'raw_outputs'])\n",
    "\n",
    "rows = list()\n",
    "formatted_results_df = results_df.copy()\n",
    "formatted_results_df['neutral'] = 1 - formatted_results_df['is_biased'] \n",
    "\n",
    "for gid, group in formatted_results_df.groupby(by='query'):\n",
    "    _targets = list(filter(lambda x: x in clazzes + ['neutral'], group.columns[group.eq(1, axis=1).any()].tolist()))  # list of positive _targets (it is multi-target)\n",
    "    minority_preds = dict()\n",
    "    if len(group) > 1:\n",
    "        sorted_ = group.sort_values(by='votes', ascending=False)\n",
    "        best = sorted_.iloc[0]\n",
    "        minority = sorted_.iloc[1:]\n",
    "        for m in minority.itertuples():\n",
    "            minority_preds[m.clazz] = {'votes': m.votes, 'steps_per_vote': m.steps}\n",
    "    else:\n",
    "        best = group.iloc[0]\n",
    "    \n",
    "    # veracity\n",
    "    if len(_targets) > 1:\n",
    "        pred = best.clazz.lower()\n",
    "        if 'neutral' in _targets:\n",
    "            veracity = '(multi-target) contradictory targets'\n",
    "        elif pred in _targets:\n",
    "            veracity = '(multi-target) partially correct'\n",
    "        else:\n",
    "            veracity = '(multi-target) incorrect'\n",
    "    elif len(targets) == 0:\n",
    "        veracity = \"no target found\"\n",
    "    else:\n",
    "        target = _targets[0].lower()\n",
    "        pred = best.clazz.lower()\n",
    "        if target in clazzes and pred == target:\n",
    "            veracity = 'true positive'\n",
    "        elif target == 'neutral' and pred == target:\n",
    "            veracity = 'true negative'\n",
    "        elif pred in clazzes and target == 'neutral':\n",
    "            veracity = 'false positive'\n",
    "        elif pred == 'neutral' and target in clazzes:\n",
    "            veracity = 'false negative'\n",
    "        elif pred != target and pred in clazzes and target in clazzes and target not in cotsc.classes:\n",
    "            veracity = 'unsupported classification'\n",
    "        elif pred != target and pred in clazzes and target in clazzes:  # when > 1 bias in cotsc\n",
    "            veracity = 'incorrect bias'\n",
    "        else:\n",
    "            pred_chunks = set(p.strip() for p in  pred.split(\",\"))\n",
    "            if target in pred_chunks:\n",
    "                veracity = \"malformed but partially correct\"\n",
    "            else:\n",
    "                veracity = \"malformed and incorrect\"\n",
    "    \n",
    "    row = Row(query=best.query, \n",
    "              majority_pred=best.clazz, \n",
    "              veracity=veracity,\n",
    "              votes=best.votes,\n",
    "              steps_per_vote='\\n\\n'.join(best.completions),\n",
    "              minority_preds=minority_preds,\n",
    "              det=best.determinism,\n",
    "              se=best.specific_aetiology,\n",
    "              nat=best.naturalness,\n",
    "              hom=best.homogeneity,\n",
    "              neutral= best.neutral,\n",
    "              raw_outputs=best.completions,\n",
    "             )\n",
    "    rows.append(row)\n",
    "\n",
    "formatted_results_df = pd.DataFrame(rows)\n",
    "formatted_results_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8914f-37f1-44a3-967d-98569fd698fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File 1: classification outputs\n",
    "path = dir_.joinpath('cotsc-outputs.xlsx')\n",
    "formatted_results_df.to_excel(path, index=False)\n",
    "\n",
    "assert path.exists(), f\"Failed to save to {path}\"\n",
    "\"Added 'cotsc-outputs.xslx' to zip. Please continue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c7c6d-4242-4ab3-b8bf-b3567e8dc9b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File 2: classification evaluation results\n",
    "path = dir_.joinpath('cotsc-results.txt')\n",
    "with open(path, 'w') as f:\n",
    "    clazzes = list(clazz_to_idx.keys())\n",
    "    num_clazzes = preds.shape[1]\n",
    "    report(clazzes, targets, preds, file=f)\n",
    "    \n",
    "assert path.exists(), f\"Failed to save to {path}\"\n",
    "\"Added 'cotsc-results.txt' to zip. Please continue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f18ed1-5673-4597-b711-0677aa62cc3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File 3: classification evaluation confusion matrix\n",
    "path = dir_.joinpath('cotsc-confusion-matrix.png')\n",
    "path_st = dir_.joinpath('cotsc-confusion-matrix-st.png')\n",
    "assert fig, \"Did you run the confusion matrix cell earlier?\"\n",
    "assert fig_st, \"Did you run the confusion matrix (single target) cell earlier?\"\n",
    "_ = fig.savefig(path)\n",
    "_ = fig_st.savefig(path_st)\n",
    "\"Added confusion matrix to zip. Please continue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63c9014-5ddc-4c5e-9c1a-7e37057a50f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File 4: cotsc configuration\n",
    "import srsly\n",
    "\n",
    "path = dir_.joinpath('cotsc-config.json')\n",
    "cotsc_config = {\n",
    "    'sampling_scheme': sampling_scheme.openai(),\n",
    "    'n_completions': cotsc.n_completions,\n",
    "    'model': cotsc.model,\n",
    "    'classes': cotsc.classes,\n",
    "}\n",
    "srsly.write_json(path, cotsc_config)\n",
    "assert path.exists(), f\"Failed to save to {path}\"\n",
    "\n",
    "\"Added cotsc configurations cotsc-config.json to zip. Please continue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff059ac6-449d-43d7-911f-6ec4a1690867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of names of files to be added to the zip\n",
    "file_names = [exemplars['data'], dataset['data']]  # toml & dataset\n",
    "file_names += list(dir_.glob('*'))\n",
    "\n",
    "print(\"These are the files that will be saved in the zip: \")\n",
    "[f.name for f in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f314a54e-0f4e-4aa4-9694-fb0a89ab3f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open a zip file in write mode\n",
    "# zip results and causation config\n",
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import panel as pn\n",
    "\n",
    "now = datetime.now().strftime(format=\"%Y-%m-%d_%H-%M-%S\")\n",
    "zfname = Path(f'cotsc-results_{now}.zip')\n",
    "with zipfile.ZipFile(zfname, 'w') as zipf:\n",
    "    for file_name in file_names:\n",
    "        zipf.write(file_name, arcname=os.path.basename(file_name))\n",
    "print(f\"Saved as {zfname}.\\nClick below to download.\")\n",
    "\n",
    "# download link for the zip.\n",
    "pn.widgets.FileDownload(file=str(zfname), filename=zfname.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
